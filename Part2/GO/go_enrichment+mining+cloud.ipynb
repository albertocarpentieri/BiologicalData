{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####building dictionary for GO parents, ancestors and children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "\n",
    "with gzip.open(\"res/go.json.gz\") as f:\n",
    "    ontology = json.load(f)\n",
    "\n",
    "#make a dict of edges,key=edge sub and value = edge obj\n",
    "#http://geneontology.org/docs/ontology-relations/\n",
    "\n",
    "parents = {}  # { term : list_of_parent_terms }\n",
    "for edge in ontology[\"graphs\"][0][\"edges\"]:\n",
    "    # select only is_a edges\n",
    "    if edge[\"pred\"] == \"is_a\":\n",
    "        parents.setdefault(edge[\"sub\"].split(\"_\")[1], []).append(edge[\"obj\"].split(\"_\")[1])\n",
    "\n",
    "        \n",
    "        \n",
    "#make dict of GO node labels, not deprecated (existing nodes only)\n",
    "nodes = []  # list of node terms\n",
    "labels = {}  # { term : definition }\n",
    "for node in ontology[\"graphs\"][0][\"nodes\"]:\n",
    "    # exclude obsolete terms\n",
    "    if \"GO_\" in node[\"id\"] and \"deprecated\" not in node[\"meta\"]:\n",
    "        nodes.append(node[\"id\"].split(\"_\")[1])\n",
    "        labels[node[\"id\"].split(\"_\")[1]] = node[\"lbl\"]\n",
    "\n",
    "# Build an ancestors dictionary\n",
    "ancestors = {}  # { term : list_of_ancestor_terms }\n",
    "for node in nodes:\n",
    "    node_ancestors = []\n",
    "    node_parents = parents.get(node)\n",
    "    # Loop parent levels until no more parents\n",
    "    while node_parents:\n",
    "        node_ancestors.extend(node_parents)\n",
    "        # Get the parents of current parents (1 level up)\n",
    "        node_parents = [term for parent in node_parents for term in parents.get(parent, [])]\n",
    "    ancestors[node] = node_ancestors\n",
    "\n",
    "\n",
    "# *** Build a dictionary for the children (similar to the ancestors one) efficiently\n",
    "children = {}  # { node : list_of_children }, leaf terms are not keys\n",
    "for node in ancestors:\n",
    "    for ancestor in ancestors[node]:\n",
    "        children.setdefault(ancestor, set()).add(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#function for taking one dataset at a time and making a list of using each line of the original dataset\n",
    "\n",
    "def get_dataset(path):\n",
    "    dataset = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            dataset.append(line[:-1])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#take the file, preprocess to get term ID, return a set of term values \n",
    "\n",
    "def gen_block(f):\n",
    "    \"\"\"\n",
    "    Parse and split the input.\n",
    "    The input must be sorted by target name, second column.\n",
    "\n",
    "    UniProtKB       A0A024R1R8      hCG_2014768             GO:0002181      PMID:21873635   IBA     PANTHER:PTN002008372|SGD:S000007246     P       HCG2014768, isoform CRA_a       hCG_2014768     protein taxon:9606      20171102        GO_Central\n",
    "    UniProtKB       A0A024RBG1      NUDT4B          GO:0003723      GO_REF:0000037  IEA     UniProtKB-KW:KW-0694    F       Diphosphoinositol polyphosphate phosphohydrolase NUDT4B NUDT4B  protein taxon:9606      20191109        UniProt\n",
    "    UniProtKB       A0A024RBG1      NUDT4B          GO:0005829      GO_REF:0000052  IDA             C       Diphosphoinositol polyphosphate phosphohydrolase NUDT4B NUDT4B  protein taxon:9606      20161204        HPA\n",
    "    \"\"\"\n",
    "    name, old_name = None, None\n",
    "    chunk = []\n",
    "    for line in f:\n",
    "        line = line.decode()\n",
    "        if line and line[0] != \"!\":\n",
    "            _, name, _, _, term, _, ec, _, namespace, protein_name = line.split(\"\\t\")[:10]\n",
    "            term = term[3:]  # remove \"GO:\" from the term ID\n",
    "            if name != old_name and old_name:\n",
    "                yield old_name, set(chunk)  # return a set as there can be repetitions, i.e. the same term with different evidence codes\n",
    "                chunk = []\n",
    "            old_name = name\n",
    "            chunk.append(term)\n",
    "    # Last line\n",
    "    if old_name:\n",
    "        yield old_name, set(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#GO association file for humans which annotates or maps the human genes to their corresponding GO definition file ids in the following way.\n",
    "#https://sajeewasp.com/bioinformatics-gene-ontology/\n",
    "\n",
    "def GO_counts(path, dataset=None):\n",
    "    if path:\n",
    "        dataset = get_dataset(path)\n",
    "    term_counts = {}\n",
    "    with gzip.open(\"res/goa_human.gaf.gz\") as f:\n",
    "        for acc, annotations in gen_block(f):\n",
    "            if acc in dataset: # if the protein of Human Go file is in the dataset passed to the function\n",
    "                for term in annotations:   # for terms in set of terms\n",
    "                    term_counts.setdefault(term, 0)  #count the frequency of terms and populate a dictionary key=term ID value=no. of counts \n",
    "                    term_counts[term] += 1\n",
    "    return term_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "###calculate enrichment\n",
    "\n",
    "def enrichment(domains, dataset):\n",
    "    go_terms_counter_dataset: dict = GO_counts(None, dataset)\n",
    "    num_go_dataset = sum(go_terms_counter_dataset.values())         #calulate sum of frequencies of terms in dataset\n",
    "    \n",
    "    go_terms_counter_human = GO_counts(\"datasets/Original_dataset5.txt\")\n",
    "    num_go_human = sum(go_terms_counter_human.values())             #calulate sum of frequencies of terms in original dataset\n",
    "    \n",
    "    #make set of similar values in both datasets\n",
    "    key_intersection = set(go_terms_counter_dataset.keys()).intersection(go_terms_counter_human.keys())   \n",
    "    \n",
    "    # Measure enrichment performing a Fisherâ€™s exact test (hypergeometric test).\n",
    "    \n",
    "    from scipy.stats import fisher_exact\n",
    "    \n",
    "    def p_value(go_terms_counter_dataset, num_go_dataset, go_terms_counter_human, num_go_human):\n",
    "        p_values = {}\n",
    "        for key in key_intersection:   #calculate p value for same keys only\n",
    "            n_term = go_terms_counter_dataset[key]\n",
    "            human_num = go_terms_counter_human[key]\n",
    "            not_dataset = num_go_dataset - n_term\n",
    "            not_human = num_go_human - human_num\n",
    "            p_value = fisher_exact([[n_term, human_num],[not_dataset, not_human]])[1]\n",
    "            p_values.setdefault(key, p_value)   #make dict of p-values\n",
    "        return p_values\n",
    "    \n",
    "    \n",
    "    go_p_values = p_value(go_terms_counter_dataset, num_go_dataset, go_terms_counter_human, num_go_human)\n",
    "    go_p_values = {k: v for k, v in sorted(go_p_values.items(), key=lambda item: item[1], reverse=False)}\n",
    "    go_p_values = dict(filter(lambda v: v[1] < 0.05, go_p_values.items()))\n",
    "    \n",
    "    \n",
    "    # *** Calculate the minimum depth (distance from the root) of each term\n",
    "    def calc_depth():\n",
    "        roots = set(nodes) - set(parents.keys())\n",
    "        depth = {}  # { term : min_depth }\n",
    "        for node in nodes:\n",
    "            c = 0  # Depth level\n",
    "            node_parents = parents.get(node)\n",
    "            while node_parents:\n",
    "                c += 1\n",
    "                if roots.intersection(set(node_parents)):  # break the loop if the root is among parents\n",
    "                    break\n",
    "                # Get the parents of current parents (1 level up)\n",
    "                node_parents = [term for parent in node_parents for term in parents.get(parent, [])]\n",
    "            depth[node] = c\n",
    "        return depth\n",
    "    \n",
    "    depths = calc_depth()\n",
    "    \n",
    "    \n",
    "    # Take into consideration the hierarchical structure of the ontologies and report only most enriched branches, i.e. high level terms.\n",
    "    import numpy as np\n",
    "    for key in go_p_values.keys():\n",
    "        go_p_values[key] = (go_p_values[key], depths[key])\n",
    "    \n",
    "    mean = np.mean([v[1] for v in go_p_values.values()])\n",
    "    \n",
    "    go_p_values = dict(filter(lambda v: v[1][1] <= 5, go_p_values.items()))\n",
    "    \n",
    "    \n",
    "    go_labeled_enriched = {}\n",
    "    for k, v in go_p_values.items():\n",
    "        go_labeled_enriched.setdefault(labels[k], int(np.log(1 / v[0])))\n",
    "    \n",
    "    \n",
    "    from matplotlib import pyplot as plt\n",
    "    from wordcloud import WordCloud\n",
    "    \n",
    "    wordcloud = WordCloud(max_font_size=200, max_words=289, width=3000, height=3000, background_color=\"white\", scale=4)\n",
    "    wordcloud.generate_from_frequencies(go_labeled_enriched)\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.xticks([],[])\n",
    "    plt.yticks([],[])\n",
    "    plt.savefig('plot/go/wordcloud_architecture_{}.pdf'.format(domains))\n",
    "    plt.axis(\"off\")\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "We need at least 1 word to plot a word cloud, got 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-53341f11c66b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m', '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'\\\\'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0menrichment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-23-fa96e2d8c9e2>\u001b[0m in \u001b[0;36menrichment\u001b[1;34m(domains, dataset)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[0mwordcloud\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_font_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m289\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackground_color\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"white\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m     \u001b[0mwordcloud\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgo_labeled_enriched\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordcloud\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"bilinear\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3b\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[1;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[0;32m    389\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrequencies\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m             raise ValueError(\"We need at least 1 word to plot a word cloud, \"\n\u001b[1;32m--> 391\u001b[1;33m                              \"got %d.\" % len(frequencies))\n\u001b[0m\u001b[0;32m    392\u001b[0m         \u001b[0mfrequencies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfrequencies\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_words\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: We need at least 1 word to plot a word cloud, got 0."
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# some JSON:\n",
    "with open('datasets/architecture_datasets.json') as json_file:\n",
    "    data : dict= json.loads(json_file.read())\n",
    "\n",
    "for k, v in data.items():\n",
    "    k = k.replace(', ', '\\\\')\n",
    "    k = k.replace(' ', '_')\n",
    "    enrichment(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q6PIF6\n",
      "P53778\n",
      "P07355\n",
      "Q8NEN9\n",
      "Q16348\n",
      "Q13402\n",
      "Q9H251\n",
      "O95049\n",
      "Q5TCQ9\n",
      "Q9C0E4\n",
      "Q9Y624\n",
      "Q5T2T1\n",
      "P42566\n",
      "P0CAP1\n",
      "P63252\n",
      "Q96JY6\n",
      "Q9NUP9\n",
      "Q5T2W1\n",
      "Q9Y6N9\n",
      "Q15599\n",
      "Q96RT1\n",
      "Q8IXQ8\n",
      "P26045\n",
      "P0CG47\n",
      "Q6IN97\n",
      "Q9UPQ7\n",
      "Q9NZW5\n",
      "Q15418\n",
      "Q9UDY2\n",
      "Q5JV73\n",
      "Q92796\n",
      "P57105\n",
      "Q86UT5\n",
      "O43572\n",
      "A8MUH7\n",
      "Q07157\n",
      "P26038\n",
      "Q14CM0\n",
      "Q6ZMN7\n",
      "Q92743\n",
      "Q8TEW0\n",
      "Q96NW7\n",
      "Q9NZN5\n",
      "Q14005\n",
      "O43464\n",
      "P61088\n",
      "P81274\n",
      "Q96QZ7\n",
      "O43157\n",
      "P04626\n",
      "Q12959\n",
      "P25054\n",
      "Q14168\n",
      "O15085\n",
      "Q15700\n",
      "P83110\n",
      "Q76G19\n",
      "O75970\n",
      "Q6ZWJ1\n",
      "Q86UL8\n",
      "Q9P227\n",
      "Q8TEU7\n",
      "P78352\n",
      "Q8NI35\n",
      "P57739\n",
      "Q9HAP6\n",
      "Q9H6Y5\n",
      "O15018\n",
      "Q68DX3\n",
      "Q8TBB1\n",
      "P83105\n",
      "Q8TEW8\n",
      "Q12923\n",
      "Q96SB4\n",
      "P61586\n",
      "P36383\n",
      "Q8N448\n",
      "O14745\n",
      "Q495M9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "f = open(\"datasets/PDB_dataset5.txt\", \"r\")\n",
    "print(f.read())\n",
    "\n",
    "##########################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
